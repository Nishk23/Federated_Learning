{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOWNKt4+wZnGYEoi8FnonET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishk23/Federated_Learning/blob/main/fedlearn_training_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "RsU4hHBBGIFQ",
        "outputId": "375eca90-94ce-440c-cc9b-c050a9e16268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.86.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (5.4.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.85.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "  Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow-federated)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.5)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.3.2)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (2.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535357 sha256=bb3522db245a3b1dd7c728065af61425575aeb77b2b495c3299f63acfa93baf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529854 sha256=0903803bb8331897889094d796d8bcad37457525450206d039d6656776d16a84\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: wrapt, typing-extensions, tensorflow-estimator, sqlalchemy, protobuf, portpicker, packaging, numpy, keras, attrs, tensorflow-probability, tensorflow-model-optimization, scipy, ml-dtypes, grpcio-tools, googleapis-common-protos, jaxlib, jax, google-vizier, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow-federated\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.32\n",
            "    Uninstalling SQLAlchemy-2.0.32:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.32\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.63.2\n",
            "    Uninstalling googleapis-common-protos-1.63.2:\n",
            "      Successfully uninstalled googleapis-common-protos-1.63.2\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "albucore 0.0.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 1.4.13 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "albumentations 1.4.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "chex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\n",
            "flax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "orbax-checkpoint 0.5.23 requires jax>=0.4.26, but you have jax 0.4.14 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\n",
            "pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.3.1+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "xarray 2024.6.0 requires packaging>=23.1, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 protobuf-4.25.4 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.84.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 typing-extensions-4.5.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "portpicker"
                ]
              },
              "id": "31203a720bfc485a828b8fe794198d3b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, Add, ReLU\n",
        "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping,LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "RU6DePrUGZZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FNWMFq8W5l1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4760e5-6fa9-4bec-b67e-79610e01d22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the CIFAR-10 dataset among the 3 clients"
      ],
      "metadata": {
        "id": "hpIFad6ryYnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Number of clients\n",
        "num_clients = 3\n",
        "\n",
        "# Function to split data among clients\n",
        "def split_data(x, y, num_clients):\n",
        "    # Shuffle the data\n",
        "    indices = np.arange(x.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Split the data\n",
        "    x_shards = np.array_split(x[indices], num_clients)\n",
        "    y_shards = np.array_split(y[indices], num_clients)\n",
        "\n",
        "    return x_shards, y_shards\n",
        "\n",
        "# Split the data\n",
        "x_train_clients, y_train_clients = split_data(x_train, y_train, num_clients)\n",
        "x_test_clients, y_test_clients = split_data(x_test, y_test, num_clients)\n",
        "\n",
        "# Verify the split\n",
        "for i in range(num_clients):\n",
        "    print(f\"Client {i+1} training data shape: {x_train_clients[i].shape}, labels shape: {y_train_clients[i].shape}\")\n",
        "    print(f\"Client {i+1} test data shape: {x_test_clients[i].shape}, labels shape: {y_test_clients[i].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeMgKhzGHs-o",
        "outputId": "417cf984-4bdb-4a6d-8d6c-6f1c33e5c1fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 training data shape: (16667, 32, 32, 3), labels shape: (16667, 1)\n",
            "Client 1 test data shape: (3334, 32, 32, 3), labels shape: (3334, 1)\n",
            "Client 2 training data shape: (16667, 32, 32, 3), labels shape: (16667, 1)\n",
            "Client 2 test data shape: (3333, 32, 32, 3), labels shape: (3333, 1)\n",
            "Client 3 training data shape: (16666, 32, 32, 3), labels shape: (16666, 1)\n",
            "Client 3 test data shape: (3333, 32, 32, 3), labels shape: (3333, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Image Classification Model"
      ],
      "metadata": {
        "id": "9-R2cOZiyatM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "def create_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create a model for each client\n",
        "client_models = [create_cnn_model() for _ in range(num_clients)]\n",
        "\n",
        "# Summary of the model\n",
        "client_models[0].summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2maxp2TLEMY",
        "outputId": "40cca79d-eb98-4de4-8c1d-eee7c831d53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73418 (286.79 KB)\n",
            "Trainable params: 73418 (286.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Federated Learning Process"
      ],
      "metadata": {
        "id": "BtGvBX5OymFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of rounds of training\n",
        "num_rounds = 5\n",
        "\n",
        "# Number of epochs per round for each client\n",
        "epochs_per_round = 3\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 32\n",
        "\n",
        "# Function to average the model weights\n",
        "def federated_averaging(client_weights):\n",
        "    avg_weights = []\n",
        "    for weights in zip(*client_weights):\n",
        "        avg_weights.append(np.mean(weights, axis=0))\n",
        "    return avg_weights\n",
        "\n",
        "# Federated learning process with progress bar\n",
        "for round_num in range(num_rounds):\n",
        "    print(f\"Round {round_num+1}/{num_rounds}\")\n",
        "\n",
        "    # Store client weights after training\n",
        "    client_weights = []\n",
        "\n",
        "    # Train each client model on its local data with tqdm progress bar\n",
        "    for i in tqdm(range(num_clients), desc=f\"Training clients for round {round_num+1}\"):\n",
        "        client_models[i].fit(x_train_clients[i], y_train_clients[i],\n",
        "                             epochs=epochs_per_round, batch_size=batch_size, verbose=0)\n",
        "        # Append the client model's weights\n",
        "        client_weights.append(client_models[i].get_weights())\n",
        "\n",
        "    # Aggregate client weights using federated averaging\n",
        "    averaged_weights = federated_averaging(client_weights)\n",
        "\n",
        "    # Set the averaged weights to all client models\n",
        "    for i in range(num_clients):\n",
        "        client_models[i].set_weights(averaged_weights)\n",
        "\n",
        "    print(\"Model weights have been averaged and updated across all clients.\\n\")\n",
        "\n",
        "print(\"Federated Learning process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQHTyylfymWR",
        "outputId": "65ce9600-b1d7-4948-d6c0-4e4a7151efd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 1: 100%|██████████| 3/3 [00:32<00:00, 10.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 2: 100%|██████████| 3/3 [00:30<00:00, 10.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 3: 100%|██████████| 3/3 [00:30<00:00, 10.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 4: 100%|██████████| 3/3 [00:30<00:00, 10.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 5: 100%|██████████| 3/3 [00:29<00:00,  9.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Federated Learning process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate and Store the Performance Metrics"
      ],
      "metadata": {
        "id": "KSrngL4fyme0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path where the JSON file should be saved\n",
        "save_path = '/content/drive/My Drive/Federated_Learning'\n",
        "os.makedirs(save_path, exist_ok=True)  # Create the directory if it doesn't exist"
      ],
      "metadata": {
        "id": "6Aakr-P-6V5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on each client's test data and store metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "for i in range(num_clients):\n",
        "    print(f\"Evaluating client {i+1}\")\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = np.argmax(client_models[i].predict(x_test_clients[i]), axis=1)\n",
        "    y_true = y_test_clients[i].flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    # Store metrics in dictionary\n",
        "    performance_metrics[f'client_{i+1}'] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"Client {i+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save the performance metrics to a JSON file in the specified location\n",
        "json_file_path = os.path.join(save_path, 'simple_CNN_client_performance_metrics.json')\n",
        "with open(json_file_path, 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=4)\n",
        "\n",
        "print(f\"Performance metrics have been saved to '{json_file_path}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o33ftPujymml",
        "outputId": "fcbe492b-c246-4cea-ae7c-9da808afb08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating client 1\n",
            "105/105 [==============================] - 0s 3ms/step\n",
            "Client 1 - Accuracy: 0.5987, Precision: 0.5992, Recall: 0.5996, F1 Score: 0.5917\n",
            "Evaluating client 2\n",
            "105/105 [==============================] - 0s 3ms/step\n",
            "Client 2 - Accuracy: 0.5977, Precision: 0.5972, Recall: 0.5990, F1 Score: 0.5904\n",
            "Evaluating client 3\n",
            "105/105 [==============================] - 0s 3ms/step\n",
            "Client 3 - Accuracy: 0.5815, Precision: 0.5808, Recall: 0.5790, F1 Score: 0.5738\n",
            "Performance metrics have been saved to '/content/drive/My Drive/Federated_Learning/simple_CNN_client_performance_metrics.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Improvement"
      ],
      "metadata": {
        "id": "CcV2JwjpymuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_rounds = 5\n",
        "epochs_per_round = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "l2_lambda = 0.001\n",
        "dropout_rate = 0.5\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")"
      ],
      "metadata": {
        "id": "ipQq0596ym18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Increase Model Complexity with Regularization"
      ],
      "metadata": {
        "id": "zLMQwerRym8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_complex_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(l2_lambda)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "w2W6mIK9ynDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models for each client\n",
        "client_models = [create_complex_cnn_model() for _ in range(num_clients)]\n",
        "\n",
        "client_models[0].summary()\n",
        "\n",
        "# Learning Rate Scheduling and Early Stopping\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bqls5Mo58HZ",
        "outputId": "8a8b7356-110b-407a-cabe-9e4ccaa134c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 30, 30, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 30, 30, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 15, 15, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 15, 15, 64)        0         \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 13, 13, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 13, 13, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 6, 6, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 4, 4, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPooli  (None, 2, 2, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 512)               2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 904586 (3.45 MB)\n",
            "Trainable params: 902666 (3.44 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated Learning Process with Enhanced Aggregation"
      ],
      "metadata": {
        "id": "ZrCbs9MiynJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def federated_averaging(client_weights):\n",
        "    avg_weights = []\n",
        "    for weights in zip(*client_weights):\n",
        "        avg_weights.append(np.mean(weights, axis=0))\n",
        "    return avg_weights\n",
        "\n",
        "# Federated Learning with Progress Bar\n",
        "for round_num in range(num_rounds):\n",
        "    print(f\"Round {round_num+1}/{num_rounds}\")\n",
        "\n",
        "    client_weights = []\n",
        "    for i in tqdm(range(num_clients), desc=f\"Training clients for round {round_num+1}\"):\n",
        "        model = client_models[i]\n",
        "        # Train the model with augmented data\n",
        "        model.fit(datagen.flow(x_train_clients[i], y_train_clients[i], batch_size=batch_size),\n",
        "                  epochs=epochs_per_round,\n",
        "                  validation_data=(x_test_clients[i], y_test_clients[i]),\n",
        "                  callbacks=[reduce_lr, early_stop],\n",
        "                  verbose=0)\n",
        "\n",
        "        # Save the trained weights\n",
        "        client_weights.append(model.get_weights())\n",
        "\n",
        "    # Federated Averaging with the client weights\n",
        "    averaged_weights = federated_averaging(client_weights)\n",
        "\n",
        "    # Update the models with the averaged weights\n",
        "    for model in client_models:\n",
        "        model.set_weights(averaged_weights)\n",
        "\n",
        "    print(\"Model weights have been averaged and updated across all clients.\\n\")\n",
        "\n",
        "print(\"Federated Learning process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA0cr4CKynP0",
        "outputId": "64475a05-cec5-4a45-c778-24373296c483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 1:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 1: 100%|██████████| 3/3 [06:42<00:00, 134.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 2:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 2:  33%|███▎      | 1/3 [02:12<04:24, 132.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 2:  67%|██████▋   | 2/3 [04:25<02:12, 132.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 2: 100%|██████████| 3/3 [06:37<00:00, 132.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 3:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Epoch 7: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 3:  33%|███▎      | 1/3 [01:32<03:05, 92.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 3:  67%|██████▋   | 2/3 [03:45<01:56, 116.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 3: 100%|██████████| 3/3 [05:57<00:00, 119.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 4:  33%|███▎      | 1/3 [02:12<04:24, 132.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 4:  67%|██████▋   | 2/3 [04:24<02:12, 132.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Epoch 10: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 4: 100%|██████████| 3/3 [06:37<00:00, 132.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 5:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Epoch 6: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 5:  33%|███▎      | 1/3 [01:19<02:39, 79.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Epoch 7: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining clients for round 5:  67%|██████▋   | 2/3 [02:52<01:27, 87.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Epoch 8: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 5: 100%|██████████| 3/3 [04:38<00:00, 92.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Federated Learning process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model on each client's test data and store metrics"
      ],
      "metadata": {
        "id": "OmBo3SjtynVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance_metrics = {}\n",
        "\n",
        "for i in range(num_clients):\n",
        "    print(f\"Evaluating client {i+1}\")\n",
        "\n",
        "    y_pred = np.argmax(client_models[i].predict(x_test_clients[i]), axis=1)\n",
        "    y_true = y_test_clients[i].flatten()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    performance_metrics[f'client_{i+1}'] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"Client {i+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save the performance metrics to a JSON file\n",
        "json_file_path = os.path.join(save_path, 'complex_CNN_client_performance_metrics.json')\n",
        "with open(json_file_path, 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=4)\n",
        "\n",
        "print(f\"Performance metrics have been saved to '{json_file_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UrDS_31ynbh",
        "outputId": "8ab8a776-ee5d-4b22-8fde-00ad5dc5cf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating client 1\n",
            "105/105 [==============================] - 1s 7ms/step\n",
            "Client 1 - Accuracy: 0.7340, Precision: 0.7437, Recall: 0.7344, F1 Score: 0.7269\n",
            "Evaluating client 2\n",
            "105/105 [==============================] - 1s 7ms/step\n",
            "Client 2 - Accuracy: 0.7177, Precision: 0.7256, Recall: 0.7181, F1 Score: 0.7083\n",
            "Evaluating client 3\n",
            "105/105 [==============================] - 1s 7ms/step\n",
            "Client 3 - Accuracy: 0.7327, Precision: 0.7466, Recall: 0.7318, F1 Score: 0.7248\n",
            "Performance metrics have been saved to '/content/drive/My Drive/Federated_Learning/comples_CNN_client_performance_metrics.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvising the model"
      ],
      "metadata": {
        "id": "iItqB1VIynhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_rounds = 5\n",
        "epochs_per_round = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "l2_lambda = 0.001\n",
        "dropout_rate = 0.5\n",
        "num_clients = 3\n",
        "\n",
        "# Advanced Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Function to encode string labels\n",
        "def encode_labels(y_train, y_test):\n",
        "    label_encoder = LabelEncoder()\n",
        "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "    y_train_int = label_encoder.fit_transform(y_train)\n",
        "    y_test_int = label_encoder.transform(y_test)\n",
        "\n",
        "    y_train_oh = one_hot_encoder.fit_transform(y_train_int.reshape(-1, 1))\n",
        "    y_test_oh = one_hot_encoder.transform(y_test_int.reshape(-1, 1))\n",
        "\n",
        "    return y_train_oh, y_test_oh, label_encoder"
      ],
      "metadata": {
        "id": "37nG2jnzyno9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Model with Residual Connections and Label Smoothing"
      ],
      "metadata": {
        "id": "lITkYZNnQT3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters, kernel_size=(3, 3), activation='relu'):\n",
        "    shortcut = x\n",
        "    x = Conv2D(filters, kernel_size, padding='same', activation=activation, kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same', activation=None, kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Adjust the shape of the shortcut to match the output if necessary\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, (1, 1), padding='same', kernel_regularizer=l2(l2_lambda))(shortcut)\n",
        "        shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = ReLU()(x)  # Add ReLU after the addition\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "def create_resnet_model():\n",
        "    inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2_lambda))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 64)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 128)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 256)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=l2(l2_lambda))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "N2qu6_Ha6M_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models for each client\n",
        "client_models = [create_resnet_model() for _ in range(num_clients)]\n",
        "\n",
        "client_models[0].summary()\n",
        "\n",
        "# Learning Rate Scheduling and Early Stopping\n",
        "def one_cycle_scheduler(epoch, max_lr=learning_rate):\n",
        "    return max_lr * (1 - epoch / (epochs_per_round * num_rounds))\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(one_cycle_scheduler)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rOLwRdyp6NN6",
        "outputId": "aafd3f52-b554-4962-ed70-7c383350d4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)         (None, 32, 32, 64)           1792      ['input_13[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_120 (B  (None, 32, 32, 64)           256       ['conv2d_108[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_48 (MaxPooli  (None, 16, 16, 64)           0         ['batch_normalization_120[0][0\n",
            " ng2D)                                                              ]']                           \n",
            "                                                                                                  \n",
            " conv2d_109 (Conv2D)         (None, 16, 16, 64)           36928     ['max_pooling2d_48[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_121 (B  (None, 16, 16, 64)           256       ['conv2d_109[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_110 (Conv2D)         (None, 16, 16, 64)           36928     ['batch_normalization_121[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " batch_normalization_122 (B  (None, 16, 16, 64)           256       ['conv2d_110[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_36 (Add)                (None, 16, 16, 64)           0         ['batch_normalization_122[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'max_pooling2d_48[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu_36 (ReLU)             (None, 16, 16, 64)           0         ['add_36[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)        (None, 16, 16, 64)           0         ['re_lu_36[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_49 (MaxPooli  (None, 8, 8, 64)             0         ['dropout_48[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_111 (Conv2D)         (None, 8, 8, 128)            73856     ['max_pooling2d_49[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_123 (B  (None, 8, 8, 128)            512       ['conv2d_111[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_112 (Conv2D)         (None, 8, 8, 128)            147584    ['batch_normalization_123[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_113 (Conv2D)         (None, 8, 8, 128)            8320      ['max_pooling2d_49[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_124 (B  (None, 8, 8, 128)            512       ['conv2d_112[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_125 (B  (None, 8, 8, 128)            512       ['conv2d_113[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_37 (Add)                (None, 8, 8, 128)            0         ['batch_normalization_124[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_125[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " re_lu_37 (ReLU)             (None, 8, 8, 128)            0         ['add_37[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)        (None, 8, 8, 128)            0         ['re_lu_37[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_50 (MaxPooli  (None, 4, 4, 128)            0         ['dropout_49[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)         (None, 4, 4, 256)            295168    ['max_pooling2d_50[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_126 (B  (None, 4, 4, 256)            1024      ['conv2d_114[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)         (None, 4, 4, 256)            590080    ['batch_normalization_126[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)         (None, 4, 4, 256)            33024     ['max_pooling2d_50[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_127 (B  (None, 4, 4, 256)            1024      ['conv2d_115[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_128 (B  (None, 4, 4, 256)            1024      ['conv2d_116[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_38 (Add)                (None, 4, 4, 256)            0         ['batch_normalization_127[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_128[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " re_lu_38 (ReLU)             (None, 4, 4, 256)            0         ['add_38[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)        (None, 4, 4, 256)            0         ['re_lu_38[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_51 (MaxPooli  (None, 2, 2, 256)            0         ['dropout_50[0][0]']          \n",
            " ng2D)                                                                                            \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)        (None, 1024)                 0         ['max_pooling2d_51[0][0]']    \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 512)                  524800    ['flatten_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_129 (B  (None, 512)                  2048      ['dense_24[0][0]']            \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)        (None, 512)                  0         ['batch_normalization_129[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 10)                   5130      ['dropout_51[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1761034 (6.72 MB)\n",
            "Trainable params: 1757322 (6.70 MB)\n",
            "Non-trainable params: 3712 (14.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Federated Learning Process with Enhanced Aggregation"
      ],
      "metadata": {
        "id": "3nAhVvIl6NUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def federated_averaging(client_weights):\n",
        "    avg_weights = []\n",
        "    for weights in zip(*client_weights):\n",
        "        avg_weights.append(np.mean(weights, axis=0))\n",
        "    return avg_weights\n",
        "\n",
        "# Assuming x_train_clients and y_train_clients are defined and contain the data for each client\n",
        "for i in range(num_clients):\n",
        "    y_train_clients[i], y_test_clients[i], label_encoder = encode_labels(y_train_clients[i], y_test_clients[i])\n",
        "\n",
        "# Federated Learning with Progress Bar and Error Handling\n",
        "for round_num in range(num_rounds):\n",
        "    print(f\"Round {round_num+1}/{num_rounds}\")\n",
        "    try:\n",
        "        client_weights = []\n",
        "        for i in tqdm(range(num_clients), desc=f\"Training clients for round {round_num+1}\"):\n",
        "            model = client_models[i]\n",
        "            # Train the model with augmented data\n",
        "            model.fit(datagen.flow(x_train_clients[i], y_train_clients[i], batch_size=batch_size),\n",
        "                      epochs=epochs_per_round,\n",
        "                      validation_data=(x_test_clients[i], y_test_clients[i]),\n",
        "                      callbacks=[lr_scheduler, early_stop],\n",
        "                      verbose=0)\n",
        "\n",
        "            # Save the trained weights\n",
        "            client_weights.append(model.get_weights())\n",
        "\n",
        "        # Federated Averaging with the client weights\n",
        "        averaged_weights = federated_averaging(client_weights)\n",
        "\n",
        "        # Update the models with the averaged weights\n",
        "        for model in client_models:\n",
        "            model.set_weights(averaged_weights)\n",
        "\n",
        "        print(\"Model weights have been averaged and updated across all clients.\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during round {round_num+1}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(\"Federated Learning process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Lmt8dWL6NaM",
        "outputId": "662eb102-1d20-458f-c0fd-884345bd7aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 1: 100%|██████████| 3/3 [10:57<00:00, 219.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 2: 100%|██████████| 3/3 [10:48<00:00, 216.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 3: 100%|██████████| 3/3 [10:48<00:00, 216.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 4: 100%|██████████| 3/3 [10:45<00:00, 215.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Round 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training clients for round 5: 100%|██████████| 3/3 [10:47<00:00, 215.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model weights have been averaged and updated across all clients.\n",
            "\n",
            "Federated Learning process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model on each client's test data and store metrics"
      ],
      "metadata": {
        "id": "sI7pQBwI6NgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on each client's test data and store metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "for i in range(num_clients):\n",
        "    print(f\"Evaluating client {i+1}\")\n",
        "    try:\n",
        "        # Generate predictions\n",
        "        y_pred = np.argmax(client_models[i].predict(x_test_clients[i]), axis=1)\n",
        "        y_true = np.argmax(y_test_clients[i], axis=1)\n",
        "\n",
        "        # Debugging Info: Print shapes of y_pred and y_true\n",
        "        # print(f\"Client {i+1}: y_pred shape: {y_pred.shape}, y_true shape: {y_true.shape}\")\n",
        "\n",
        "        # Check for matching number of samples\n",
        "        if len(y_pred) != len(y_true):\n",
        "            raise ValueError(f\"Inconsistent number of samples: y_pred has {len(y_pred)}, y_true has {len(y_true)}\")\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, average='macro')\n",
        "        recall = recall_score(y_true, y_pred, average='macro')\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        performance_metrics[f'client_{i+1}'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "\n",
        "        print(f\"Client {i+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during evaluation of client {i+1}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Save the performance metrics to a JSON file\n",
        "save_path = '/content/drive/My Drive/Federated_Learning'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "json_file_path = os.path.join(save_path, 'enhanced_resnet_client_performance_metrics.json')\n",
        "with open(json_file_path, 'w') as f:\n",
        "    json.dump(performance_metrics, f, indent=4)\n",
        "\n",
        "print(f\"Performance metrics have been saved to '{json_file_path}'\")\n"
      ],
      "metadata": {
        "id": "s2ScRVuA6Nmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b664c0c-6655-4868-c7b4-c2a5f2f65752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating client 1\n",
            "105/105 [==============================] - 1s 13ms/step\n",
            "Client 1 - Accuracy: 0.8053, Precision: 0.8234, Recall: 0.8087, F1 Score: 0.8038\n",
            "Evaluating client 2\n",
            "105/105 [==============================] - 1s 13ms/step\n",
            "Client 2 - Accuracy: 0.8026, Precision: 0.8230, Recall: 0.8007, F1 Score: 0.7977\n",
            "Evaluating client 3\n",
            "105/105 [==============================] - 2s 15ms/step\n",
            "Client 3 - Accuracy: 0.8098, Precision: 0.8290, Recall: 0.8088, F1 Score: 0.8062\n",
            "Performance metrics have been saved to '/content/drive/My Drive/Federated_Learning/enhanced_resnet_client_performance_metrics.json'\n"
          ]
        }
      ]
    }
  ]
}